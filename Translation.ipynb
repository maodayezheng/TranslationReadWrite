{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Background "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all satisfy with those models which dynamically encode (or generate) images. For example in DRAWRNN the model can perform human like digits recognition and generation. And in Attent Inference and Repeat the model can attent to specific part of image and then generate caption according to that part. The common feature among these model is that, a controller neural network controls attention based operations to encode inputs or update hidden states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However when encoding information from text the most preferential way is still using RNN to sequentially process input tokens one at a time.  Can we develop a architecture such that the model can \"learn\" its own mechanism to encode text ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project explores the possibility of using the RNN + attention opeartions architecture to develop a more flexible encoder for encoding text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Avoid Ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should the model produces different encoding for \"He thinks Bob is good\" and \"Bob thinks he is good \" ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Varied Length Input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should the designed opeartion handles varied length problem ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is trained on the sentences with maximum length 30, the model should be able to apply to a sentence whose length is more than 30 without retrain model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Flexibility (Not for Today Discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we expect more flexibility from model ? For example, make the encoding time step and RNN length independent from input length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "( If you are interesting in can find out our draft paper )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Vanilla Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we need to calculate $n$ different score for a sentence $\\boldsymbol{x}_{1}, ... , \\boldsymbol{x}_{n}$ with length $n$. This can be done by following equation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " \\alpha_{i} = Relu(tanh(\\boldsymbol{W}\\boldsymbol{h}))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $i \\in \\{1, 2, 3, ..., n\\}$, $\\boldsymbol{h}$ is hidden state from RNN. When perform reading we simplely do a weighted sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\boldsymbol{r} = \\sum_{i=1}^{n} \\alpha_{i}\\boldsymbol{E}\\boldsymbol{x}_{i}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vanilla approach can affective address the issue of ambiguity, since when swap the position of any $\\boldsymbol{x}_{i}$ their associate score will also change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|    Encoder   | Decoder |   Embedding Params   | RNN Params | Attention Param | location Params|BLEU |\n",
    "| ------| :-----:|:----:|:-------------------:| :---------:|:---------------:| :---------------:|:---:|\n",
    "|  RNN| RNNSearch | 56.8M |  14.9M| 524.8K | -|18| \n",
    "|  Vanilla | RNNSearch | 56.8M |14.9M| 524.8K |51K| 18|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this method does not address the issues of varied length input. Can we do better ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Location Based Mechanisms (Training In Progressing ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact each words in a sentence have their own index value 1 to n. We can normalize these index value by the sentence's length $n$ to produce $n$ different positional indicator $p_{i}$. Therefore no matter how long the sentences are their location indicator is also in between 0 and 1. \n",
    "\n",
    "Now the neural network can emits a start location indicator $s$ and stop location indicator $e$ and compute the positional score of each words as \n",
    "\\begin{equation}\n",
    " \\alpha_{i} = Relu(p_{i} - s ) Relu(e- p_{i})\n",
    "\\end{equation}\n",
    "or \n",
    "\\begin{equation}\n",
    "\\alpha_{i} = Relu( 1- |\\frac{2}{d}(p_{i} - s) - 1|)\n",
    "\\end{equation}\n",
    "Where $d = e - s$, we now shortly refer the first method as Relu-product the second as Relu-interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|    Encoder   | Decoder |   Embedding Params   | RNN Params | Attention Param | location Params|BLEU |\n",
    "| ------| :-----:|:----:|:-------------------:| :---------:|:---------------:| :---------------:|:---:|\n",
    "|  RNN| RNNSearch | 56.8M |  14.9M| 524.8K | -|18| \n",
    "|  Vanilla | RNNSearch | 56.8M |14.9M| 524.8K |51.2K| 18|\n",
    "|  Relu-prod | RNNSearch | 56.8M |14.9M| 524.8K |2.0K| 18|\n",
    "|  Relu-inter | RNNSearch | 56.8M |14.9M| 524.8K |0.5K| 18|\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
